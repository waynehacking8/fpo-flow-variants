%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NeurIPS-style Poster
% Comparative Analysis of Flow Schedules in FPO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[final,hyperref={pdfpagelabels=false}]{beamer}
\usepackage[orientation=portrait,size=a0,scale=1.4]{beamerposter}
\usetheme{confposter}

%-----------------------------------------------------------
% Packages
%-----------------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{tcolorbox}

%-----------------------------------------------------------
% Colors (NeurIPS-inspired)
%-----------------------------------------------------------
\definecolor{neuripsblue}{RGB}{0, 51, 102}
\definecolor{neuripsred}{RGB}{180, 50, 50}
\definecolor{neuripsgray}{RGB}{100, 100, 100}
\definecolor{neuripsgreen}{RGB}{34, 139, 34}
\definecolor{lightblue}{RGB}{230, 240, 250}
\definecolor{lightyellow}{RGB}{255, 250, 230}
\definecolor{lightgreen}{RGB}{230, 250, 230}
\definecolor{lightred}{RGB}{255, 235, 235}

\setbeamercolor{block title}{fg=white,bg=neuripsblue}
\setbeamercolor{block body}{fg=black,bg=white}
\setbeamercolor{block alerted title}{fg=white,bg=neuripsred}
\setbeamercolor{block alerted body}{fg=black,bg=lightred}

%-----------------------------------------------------------
% Custom boxes
%-----------------------------------------------------------
\newtcolorbox{keybox}{
    colback=lightyellow,
    colframe=neuripsblue,
    boxrule=2pt,
    arc=4pt,
    fonttitle=\bfseries\Large,
    title=Key Finding
}

\newtcolorbox{resultbox}{
    colback=lightgreen,
    colframe=neuripsgreen,
    boxrule=2pt,
    arc=4pt
}

%-----------------------------------------------------------
% Title, Authors
%-----------------------------------------------------------
\title{\Huge\textbf{Comparative Analysis of Flow Schedules in\\Flow Policy Optimization for Robot Learning}}

\author{\Large Wei-Cheng Chiu$^1$, Shang-Yen Lee$^1$, Yu-Tang Chang$^1$, Li-Chen Kao$^1$}

\institute{\large $^1$National Taiwan University, Department of Computer Science and Information Engineering}

%-----------------------------------------------------------
% Document
%-----------------------------------------------------------
\begin{document}
\begin{frame}[t]

%-----------------------------------------------------------
% Title Row
%-----------------------------------------------------------
\begin{columns}[t]
\begin{column}{0.98\textwidth}
\begin{beamercolorbox}[wd=\textwidth,center]{title}
\usebeamerfont{title}\inserttitle\\[0.5ex]
\usebeamerfont{author}\insertauthor\\[0.3ex]
\usebeamerfont{institute}\insertinstitute
\end{beamercolorbox}
\end{column}
\end{columns}

\vspace{1cm}

%-----------------------------------------------------------
% Main Content - Three Columns
%-----------------------------------------------------------
\begin{columns}[t]

%=========================================================
% LEFT COLUMN
%=========================================================
\begin{column}{0.31\textwidth}

%-----------------------------------------------------------
% Motivation
%-----------------------------------------------------------
\begin{block}{\Large 1. Motivation \& Research Question}

\textbf{Flow Policy Optimization (FPO)} replaces Gaussian policies with flow-based generative models for RL.

\vspace{0.5cm}

\textbf{The Problem:} The original FPO paper uses Optimal Transport (OT) schedule, but \textcolor{neuripsred}{\textbf{never compared}} with other schedules from generative modeling:

\begin{itemize}[leftmargin=*]
    \item Variance Preserving (VP) - DDPM
    \item Variance Exploding (VE) - Score SDE
    \item Cosine - Improved DDPM
\end{itemize}

\vspace{0.5cm}

\begin{tcolorbox}[colback=lightblue,colframe=neuripsblue,boxrule=2pt]
\centering\Large\textbf{Research Question}\\[0.3cm]
\large Which flow schedule works best for reinforcement learning? And why?
\end{tcolorbox}

\end{block}

\vspace{1cm}

%-----------------------------------------------------------
% Method
%-----------------------------------------------------------
\begin{block}{\Large 2. Flow Schedules Compared}

Flow matching interpolates noise $x_0$ to action $x_1$:
\begin{equation*}
x_t = \alpha_t \cdot x_1 + \sigma_t \cdot x_0
\end{equation*}

\vspace{0.3cm}

\renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}p{2.8cm}p{4.5cm}p{4cm}@{}}
\toprule
\textbf{Schedule} & \textbf{Coefficients} & \textbf{Path Geometry} \\
\midrule
\rowcolor{lightgreen}
\textbf{OT} & $\alpha_t = 1-t,\ \sigma_t = t$ & Straight line \\
\textbf{VP} & $\alpha_t = \cos(\frac{\pi t}{2})$ & Curved (spherical) \\
\textbf{Cosine} & $\alpha_t = \cos^2(\frac{\pi t}{2})$ & Curved (smooth) \\
\rowcolor{lightred}
\textbf{VE} & $\sigma_t = 0.01 \cdot 8000^t$ & Explosive \\
\bottomrule
\end{tabular}

\vspace{0.8cm}

\textbf{Key Insight:} OT has \textcolor{neuripsgreen}{\textbf{constant}} velocity target $v = x_1 - x_0$, while others have \textbf{time-varying} targets.

\end{block}

\vspace{1cm}

%-----------------------------------------------------------
% Environments
%-----------------------------------------------------------
\begin{block}{\Large 3. Experimental Setup}

\textbf{4 Robot Control Tasks} (IsaacGym / MJX):

\vspace{0.5cm}

\renewcommand{\arraystretch}{1.4}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Environment} & \textbf{Act Dim} & \textbf{Type} \\
\midrule
HumanoidGetup & 21 & Goal-directed \\
Go1 Getup & 12 & Goal-directed \\
Go1 Joystick & 12 & Tracking \\
\rowcolor{lightyellow}
Go1 Handstand & 12 & \textbf{Multimodal} \\
\bottomrule
\end{tabular}

\vspace{0.5cm}

\textbf{Training:} 10M steps, 2048 parallel envs, JAX/Brax

\end{block}

\end{column}

%=========================================================
% MIDDLE COLUMN
%=========================================================
\begin{column}{0.31\textwidth}

%-----------------------------------------------------------
% Main Results
%-----------------------------------------------------------
\begin{block}{\Large 4. Main Results: OT Wins All Tasks}

\begin{center}
\includegraphics[width=0.95\textwidth]{../plots_multienv/final_performance_comparison.png}
\end{center}

\vspace{0.3cm}

\begin{resultbox}
\centering\large
\textbf{OT outperforms VP by +2\% to +183\%}\\
(largest gap in multimodal Handstand task)
\end{resultbox}

\vspace{0.5cm}

\textbf{Normalized Performance Heatmap:}

\begin{center}
\includegraphics[width=0.85\textwidth]{../plots_multienv/performance_heatmap.png}
\end{center}

\vspace{0.3cm}

\renewcommand{\arraystretch}{1.3}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Task} & \textbf{OT} & \textbf{VP} & \textbf{Cosine} \\
\midrule
HumanoidGetup & \cellcolor{lightgreen}100\% & 97.7\% & 98.0\% \\
Go1 Getup & \cellcolor{lightgreen}100\% & \cellcolor{lightred}47.4\% & 54.8\% \\
Go1 Joystick & \cellcolor{lightgreen}100\% & 91.1\% & 80.0\% \\
Go1 Handstand & \cellcolor{lightgreen}100\% & \cellcolor{lightred}35.3\% & 41.0\% \\
\bottomrule
\end{tabular}

\end{block}

\vspace{1cm}

%-----------------------------------------------------------
% VE Failure
%-----------------------------------------------------------
\begin{alertblock}{\Large 5. Why Variance Exploding (VE) Fails}

\textbf{VE produced NaN in 100\% of experiments!}

\vspace{0.5cm}

\begin{center}
\includegraphics[width=0.9\textwidth]{../plots_analysis/ve_failure_comprehensive.png}
\end{center}

\vspace{0.3cm}

\textbf{Root Cause: Gradient Explosion}

\begin{equation*}
\frac{d\sigma}{dt}\bigg|_{t=1} = 80 \times \ln(8000) \approx \textcolor{neuripsred}{\mathbf{719}}
\end{equation*}

\vspace{0.3cm}

\begin{itemize}[leftmargin=*]
    \item RL actions bounded: $a \in [-1, 1]$
    \item VE velocity target: $\|v\| \approx 700$
    \item $\Rightarrow$ \textbf{Scale mismatch causes NaN}
\end{itemize}

\vspace{0.3cm}

\begin{tcolorbox}[colback=lightred,colframe=neuripsred,boxrule=2pt]
\centering\large
\textbf{Conclusion:} VE is fundamentally\\incompatible with bounded action spaces
\end{tcolorbox}

\end{alertblock}

\end{column}

%=========================================================
% RIGHT COLUMN
%=========================================================
\begin{column}{0.31\textwidth}

%-----------------------------------------------------------
% FPO vs PPO
%-----------------------------------------------------------
\begin{block}{\Large 6. FPO vs PPO: Task Dependency}

\begin{center}
\includegraphics[width=0.95\textwidth]{../plots_multienv/fpo_vs_ppo_summary.png}
\end{center}

\vspace{0.3cm}

\renewcommand{\arraystretch}{1.4}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Task} & \textbf{FPO-OT} & \textbf{PPO} & \textbf{$\Delta$} \\
\midrule
HumanoidGetup & 4201.9 & 2910.0 & \cellcolor{lightgreen}\textbf{+44\%} \\
Go1 Getup & 18.29 & 12.50 & \cellcolor{lightgreen}\textbf{+46\%} \\
Go1 Handstand & 3.34 & 1.78 & \cellcolor{lightgreen}\textbf{+87\%} \\
Go1 Joystick & 4.39 & 16.94 & \cellcolor{lightred}\textbf{-74\%} \\
\bottomrule
\end{tabular}

\vspace{0.5cm}

\begin{keybox}
\centering
\textbf{Multimodal tasks} $\rightarrow$ FPO wins (+87\%)\\
\textbf{Unimodal tracking} $\rightarrow$ PPO wins (+285\%)
\end{keybox}

\end{block}

\vspace{1cm}

%-----------------------------------------------------------
% Multimodality Analysis
%-----------------------------------------------------------
\begin{block}{\Large 7. Why FPO Excels at Multimodal Tasks}

\textbf{Go1 Handstand:} Robot can flip \textcolor{blue}{\textbf{LEFT}} or \textcolor{red}{\textbf{RIGHT}}

\begin{center}
\includegraphics[width=0.95\textwidth]{../plots_analysis/go1_handstand_multimodal_explanation.png}
\end{center}

\vspace{0.3cm}

\textbf{The Mode-Averaging Problem:}

\begin{itemize}[leftmargin=*]
    \item \textbf{PPO} (Gaussian): Averages both modes $\rightarrow$ outputs \textbf{zero} $\rightarrow$ robot doesn't flip $\rightarrow$ \textcolor{neuripsred}{\textbf{FAILS}}
    \item \textbf{FPO} (Flow): Captures both peaks $\rightarrow$ samples valid action $\rightarrow$ \textcolor{neuripsgreen}{\textbf{SUCCEEDS}}
\end{itemize}

\end{block}

\vspace{1cm}

%-----------------------------------------------------------
% Conclusions
%-----------------------------------------------------------
\begin{block}{\Large 8. Conclusions \& Recommendations}

\begin{enumerate}[leftmargin=*,label=\textbf{\arabic*.}]
    \item \textcolor{neuripsgreen}{\textbf{Use Optimal Transport}} for flow-based RL
    \begin{itemize}
        \item Constant velocity $\rightarrow$ simplest regression
        \item Straight path $\rightarrow$ shortest trajectory
        \item Best performance across ALL tasks
    \end{itemize}

    \vspace{0.3cm}

    \item \textcolor{neuripsred}{\textbf{Avoid VE schedule}} in RL
    \begin{itemize}
        \item $\sigma_{max}=80$ causes gradient explosion
        \item Incompatible with bounded actions
    \end{itemize}

    \vspace{0.3cm}

    \item \textbf{Choose algorithm by task type}
    \begin{itemize}
        \item Multimodal $\rightarrow$ FPO (up to +87\%)
        \item Unimodal $\rightarrow$ PPO (more efficient)
    \end{itemize}
\end{enumerate}

\vspace{0.5cm}

\begin{tcolorbox}[colback=lightblue,colframe=neuripsblue,boxrule=3pt,arc=6pt]
\centering\Large
\textbf{Take-Home Message}\\[0.3cm]
\large
Flow schedules designed for image generation\\
\textbf{do not transfer directly} to robot learning.\\
OT is the gold standard for FPO.
\end{tcolorbox}

\end{block}

\vspace{1cm}

%-----------------------------------------------------------
% References & QR Code
%-----------------------------------------------------------
\begin{block}{\Large References}
\small
\begin{enumerate}[leftmargin=*]
    \item Wang et al. (2024). Flow Policy Optimization. \textit{arXiv}
    \item Lipman et al. (2023). Flow Matching. \textit{ICLR}
    \item Ho et al. (2020). DDPM. \textit{NeurIPS}
    \item Song et al. (2021). Score SDE. \textit{ICLR}
\end{enumerate}
\end{block}

\end{column}

\end{columns}

\end{frame}
\end{document}
