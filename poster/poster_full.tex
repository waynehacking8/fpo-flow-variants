%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NeurIPS-style Poster - Portrait A0
% Complete Version with All Important Figures
% Including SkillMimic Failure Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a0paper,portrait]{tikzposter}

%-----------------------------------------------------------
% Packages
%-----------------------------------------------------------
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multicol}

%-----------------------------------------------------------
% Theme & Colors
%-----------------------------------------------------------
\usetheme{Default}
\useblockstyle{Basic}

% Reduce block spacing
\tikzposterlatexaffectionproofoff


\definecolor{neuripsblue}{RGB}{0, 51, 102}
\definecolor{neuripsred}{RGB}{180, 50, 50}
\definecolor{neuripsgreen}{RGB}{34, 139, 34}
\definecolor{neuripsorange}{RGB}{255, 140, 0}
\definecolor{lightyellow}{RGB}{255, 250, 230}
\definecolor{lightgreen}{RGB}{230, 250, 230}
\definecolor{lightred}{RGB}{255, 235, 235}
\definecolor{lightblue}{RGB}{230, 240, 250}
\definecolor{lightorange}{RGB}{255, 245, 230}

% Custom color style
\definecolorstyle{NeurIPS}{
    \definecolor{colorOne}{RGB}{0, 51, 102}
    \definecolor{colorTwo}{RGB}{230, 230, 230}
    \definecolor{colorThree}{RGB}{255, 255, 255}
}{
    \colorlet{backgroundcolor}{white}
    \colorlet{framecolor}{colorOne}
    \colorlet{titlefgcolor}{white}
    \colorlet{titlebgcolor}{colorOne}
    \colorlet{blocktitlebgcolor}{colorOne}
    \colorlet{blocktitlefgcolor}{white}
    \colorlet{blockbodybgcolor}{white}
    \colorlet{blockbodyfgcolor}{black}
}
\usecolorstyle{NeurIPS}

%-----------------------------------------------------------
% Title - make title area taller
%-----------------------------------------------------------
\settitle{
    \centering
    \vbox{
        \centering
        \vspace{0.8cm}
        {\LARGE \color{white} \textbf{Comparative Analysis of Flow Schedules in Flow Policy Optimization for Robot Learning} \par}
        \vspace{0.8cm}
        {\large \color{white} Wei-Cheng Chiu, Shang-Yen Lee, Yu-Tang Chang, Li-Chen Kao \par}
        \vspace{0.3cm}
        {\normalsize \color{white} National Taiwan University, Department of CSIE \par}
        \vspace{0.8cm}
    }
}
\title{Comparative Analysis of Flow Schedules in Flow Policy Optimization for Robot Learning}
\author{Wei-Cheng Chiu, Shang-Yen Lee, Yu-Tang Chang, Li-Chen Kao}
\institute{National Taiwan University, Department of CSIE}

%-----------------------------------------------------------
% Document
%-----------------------------------------------------------
\begin{document}
\maketitle

\begin{columns}

%=========================================================
% LEFT COLUMN
%=========================================================
\column{0.33}

%-----------------------------------------------------------
% Motivation
%-----------------------------------------------------------
\block{\large 1. Motivation and Research Question}{
    \textbf{Flow Policy Optimization (FPO)} replaces Gaussian policies with flow-based generative models for RL.

    \vspace{0.3cm}

    \textbf{Why Flow-based Policy?}
    \begin{itemize}[leftmargin=*]
        \item Can represent \textbf{multimodal} action distributions
        \item More expressive than unimodal Gaussian (PPO)
    \end{itemize}

    \vspace{0.3cm}

    \textbf{The Problem:} Original FPO uses OT schedule, but \textcolor{neuripsred}{\textbf{never compared}} with other schedules:
    \begin{itemize}[leftmargin=*]
        \item Variance Preserving (VP), Variance Exploding (VE), Cosine
    \end{itemize}

    \vspace{0.3cm}

    \coloredbox[bgcolor=lightblue,framecolor=neuripsblue]{
        \textbf{Research Questions:}\\
        1. Which flow schedule works best for RL?\\
        2. When does FPO outperform PPO?\\
        3. Why does FPO fail on imitation tasks?
    }
}

%-----------------------------------------------------------
% Method
%-----------------------------------------------------------
\block{\large 2. Flow Schedules Compared}{
    \textbf{Flow Matching} learns to transform noise $x_0$ into action $x_1$:
    \begin{equation*}
    x_t = \alpha_t \cdot x_1 + \sigma_t \cdot x_0, \quad t \in [0, 1]
    \end{equation*}

    \vspace{0.3cm}

    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{lll}
    \toprule
    \textbf{Schedule} & \textbf{Coefficients} & \textbf{Velocity} \\
    \midrule
    \textbf{OT} & $\alpha_t = 1-t,\ \sigma_t = t$ & \textbf{Constant} \\
    VP & $\alpha_t = \cos(\frac{\pi t}{2})$ & Time-varying \\
    Cosine & $\alpha_t = \cos^2(\frac{\pi t}{2})$ & Time-varying \\
    VE & $\sigma_t = 0.01 \cdot 8000^t$ & \textbf{Explodes!} \\
    \bottomrule
    \end{tabular}

    \vspace{0.3cm}

    \textbf{Key Insight:} OT has \textcolor{neuripsgreen}{\textbf{constant}} velocity target $v = x_1 - x_0$, while others have \textbf{time-varying} targets that are harder to learn.
}

%-----------------------------------------------------------
% Experimental Setup
%-----------------------------------------------------------
\block{\large 3. Experimental Setup}{
    \textbf{5 Robot Control Tasks:}

    \vspace{0.2cm}

    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Task} & \textbf{Dim} & \textbf{Type} & \textbf{Platform} \\
    \midrule
    HumanoidGetup & 21 & Goal & MJX \\
    Go1 Getup & 12 & Goal & Brax \\
    Go1 Joystick & 12 & Tracking & Brax \\
    Go1 Handstand & 12 & Multimodal & Brax \\
    SkillMimic & 28 & Imitation & IsaacGym \\
    \bottomrule
    \end{tabular}

    \vspace{0.2cm}

    \textbf{Training:} 10M steps, 2048 parallel envs
}

%-----------------------------------------------------------
% SNR Analysis
%-----------------------------------------------------------
\block{\large 4. Theoretical Analysis: SNR}{
    \textbf{Signal-to-Noise Ratio:} $\text{SNR}(t) = \alpha_t / \sigma_t$

    \begin{center}
    \includegraphics[width=0.95\linewidth]{figures/flow_snr_comparison.png}
    \end{center}

    \textbf{Key:} VE's SNR drops to \textbf{1/80} at $t=1$ -- signal overwhelmed!
}

%=========================================================
% MIDDLE COLUMN
%=========================================================
\column{0.34}

%-----------------------------------------------------------
% Main Results
%-----------------------------------------------------------
\block{\large 5. Main Results: OT Wins All Tasks}{
    \begin{center}
    \includegraphics[width=0.95\linewidth]{figures/final_performance_comparison.png}
    \end{center}

    \coloredbox[bgcolor=lightgreen,framecolor=neuripsgreen]{
        \centering \textbf{OT outperforms VP by +2\% to +183\%}
    }

    \vspace{0.3cm}

    \textbf{Normalized Performance Heatmap:}
    \begin{center}
    \includegraphics[width=0.9\linewidth]{figures/performance_heatmap.png}
    \end{center}
}

%-----------------------------------------------------------
% Training Curves
%-----------------------------------------------------------
\block{\large 6. Training Dynamics}{
    \begin{center}
    \includegraphics[width=0.92\linewidth]{figures/training_curves_comparison.png}
    \end{center}

    \textbf{Observations:}
    \begin{itemize}[leftmargin=*]
        \item OT: Faster learning + higher final reward
        \item VP/Cosine: Slower, lower asymptotic reward
    \end{itemize}
}

%-----------------------------------------------------------
% VE Failure
%-----------------------------------------------------------
\block{\large 7. Why VE Fails (100\% NaN)}{
    \begin{center}
    \includegraphics[width=0.88\linewidth]{figures/ve_failure_comprehensive.png}
    \end{center}

    \textbf{Gradient Explosion:} $\frac{d\sigma}{dt}|_{t=1} \approx \textcolor{neuripsred}{\mathbf{719}}$

    \begin{itemize}[leftmargin=*]
        \item RL actions bounded: $a \in [-1, 1]$
        \item VE velocity: $\|v\| \approx 700$ -- Scale mismatch!
    \end{itemize}

    \coloredbox[bgcolor=lightred,framecolor=neuripsred]{
        \centering VE is \textbf{incompatible} with bounded action RL
    }
}

%=========================================================
% RIGHT COLUMN
%=========================================================
\column{0.33}

%-----------------------------------------------------------
% FPO vs PPO
%-----------------------------------------------------------
\block{\large 8. FPO vs PPO: Task Dependency}{
    \begin{center}
    \includegraphics[width=0.92\linewidth]{figures/fpo_vs_ppo_summary.png}
    \end{center}

    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lrrr}
    \toprule
    \textbf{Task} & \textbf{FPO} & \textbf{PPO} & \textbf{Gap} \\
    \midrule
    HumanoidGetup & 4202 & 2910 & \textcolor{neuripsgreen}{\textbf{+44\%}} \\
    Go1 Getup & 18.3 & 12.5 & \textcolor{neuripsgreen}{\textbf{+46\%}} \\
    Go1 Handstand & 3.34 & 1.78 & \textcolor{neuripsgreen}{\textbf{+87\%}} \\
    Go1 Joystick & 4.39 & 16.9 & \textcolor{neuripsred}{\textbf{-74\%}} \\
    \bottomrule
    \end{tabular}

    \coloredbox[bgcolor=lightyellow,framecolor=neuripsblue]{
        \centering
        \textbf{Multimodal:} FPO wins (+87\%)\\
        \textbf{Unimodal:} PPO wins (+285\%)
    }
}

%-----------------------------------------------------------
% Multimodality
%-----------------------------------------------------------
\block{\large 9. FPO Excels at Multimodal Tasks}{
    \textbf{Go1 Handstand:} Flip LEFT or RIGHT -- both valid!

    \begin{center}
    \includegraphics[width=0.92\linewidth]{figures/go1_handstand_multimodal_explanation.png}
    \end{center}

    \textbf{Mode-Averaging:} PPO averages modes to zero, \textcolor{neuripsred}{FAILS}\\
    \textbf{Flow Policy:} FPO captures both peaks, \textcolor{neuripsgreen}{WINS}
}

%-----------------------------------------------------------
% SkillMimic Failure Analysis
%-----------------------------------------------------------
\block{\large 10. Why FPO Fails on SkillMimic}{
    \coloredbox[bgcolor=lightorange,framecolor=neuripsorange]{
        \centering\textbf{Case Study: Motion Imitation}
    }

    \textbf{Result:} FPO = 0.2 vs PPO = 22-24

    \vspace{0.2cm}

    \textbf{Root Cause: Ratio Mismatch}

    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{ll}
    \toprule
    \textbf{PPO} & $r = \pi_\text{new}(a|s) / \pi_\text{old}(a|s)$ \\
    & \textit{Policy preference change} \\
    \midrule
    \textbf{FPO} & $r = \exp(\text{MSE}_\text{old} - \text{MSE}_\text{new})$ \\
    & \textit{Prediction accuracy change} \\
    \bottomrule
    \end{tabular}

    \vspace{0.2cm}

    \begin{itemize}[leftmargin=*]
        \item Imitation needs \textbf{precise action matching}
        \item FPO ratio $\neq$ action likelihood
    \end{itemize}

    \coloredbox[bgcolor=lightorange,framecolor=neuripsorange]{
        \centering \textbf{FPO unsuitable for imitation learning}
    }
}

%-----------------------------------------------------------
% Conclusions
%-----------------------------------------------------------
\block{\large 11. Conclusions}{
    \begin{enumerate}[leftmargin=*]
        \item \textcolor{neuripsgreen}{\textbf{Use OT}} for flow-based RL -- best on all tasks
        \item \textcolor{neuripsred}{\textbf{Avoid VE}} -- gradient explosion
        \item \textbf{Choose by task:} Multimodal $\rightarrow$ FPO, Unimodal/Imitation $\rightarrow$ PPO
    \end{enumerate}

    \coloredbox[bgcolor=lightblue,framecolor=neuripsblue]{
        \centering\large
        \textbf{Take-Home Message}\\[0.1cm]
        \normalsize
        Flow schedules from image generation\\
        \textbf{do not transfer} to robot learning.\\
        \textbf{OT is the gold standard for FPO.}
    }
}

\end{columns}

\end{document}
